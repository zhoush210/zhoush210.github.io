---
layout: post
title:  "PaLM-E"
subtitle: "PaLM-E: An Embodied Multimodal Language Model"
date:   2023-05-14 16:19:00 +0800
author:     "ZhouSh"
header-img: "img/in_post/PaLM-E/head.png"
header-mask: 0.5
tags:
    - 论文笔记
---
- 论文：[PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/pdf/2303.03378.pdf)

- 官网：[palm-e.github.io](https://palm-e.github.io/)

> 聊天时 yiyi推荐的论文。没接触过这块，所以不注重细节，理个大概，弄清楚来龙去脉即可

这篇论文的名字是 "**具身多模态语言模型**"（Embodied Multimodal Language Model）$^{[1]}$：

1. **具身（Embodied）**：表示该模型接入了机器人，具有了身体。
2. **多模态（Multimodal）**：表示该模型的**输入**是多模态的，包括文本（text）、图像（visual）、连续状态（continuous state estimation）。
3. **语言（Language）**：表示该模型的输出只有文本。但是输出文本已经不再限制于自然语言，已经被玩出花来了。因为代码本身也是文本，所以模型的输出可以是一段**代码**，代码执行完之后可以是一段机器人可识别的指令；也可以模型直接输出机器人可识别的**指令**；这样模型的**输出结果就可以操作机器人**。

最近OpenAI发布的Chat-GPT也是输入为多模态，输出为纯文本。
## Abstract
1. 提出了一种具身语言模型，可以直接将真实世界的连续传感器模态输入语言模型中，从而建立**单词和感知**之间的联系。
2. 模型的输入是多模态句子，包括视觉、连续状态估计、文本输入编码。
3. 用预训练的**大语言模型**进行**端到端**的训练，我们可以针对多种具身任务进行训练（包括机器人操作规划、视觉问答、字幕生成）。

> “端到端”（End-to-End）指系统将输入数据直接作为输入，通过一系列处理层（例如神经网络）进行处理，并直接输出最终结果，而不需要手动对输入数据进行特征提取或手动设计中间处理步骤。

##  Introduction
1. 大语言模型（LLM）在各个领域表现出强大的**推理能力**，包括对话、分步推理、数学问题解决、代码编写。然而现实世界中此类推理模型的**局限性**在于一个基础问题：如何将**单词和感知**联系起来。
2. 本文提出了具身语言模型，它直接接收传感器模态的连续输入，使语言模型能够为现实世界中的顺序决策做出更为基础的推断。

## Related Work

1. **通用视觉语言模型**（General vision-language modeling）：VLM能够同时理解图像和文本，并且可以应用于诸如视觉问答、字幕、光学字符识别、物体检测等任务。整合图像的方法各不相同。
2. **动作输出模型**（Actions-output models）：之前的工作侧重于将具体环境中的视觉和语言输入与直接行动预测目标相结合。
# 参考
- [[1] PaLM-E: 具身多模态语言模型](https://zhuanlan.zhihu.com/p/615879292)
- [论文翻译](https://zhuanlan.zhihu.com/p/613316732)